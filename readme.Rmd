---
title: "MOW - REPORT"
author: "Michał Sitko, Martyna Kania"
subtitle: Ensemble model - applying any classification algorithm to bootstrap samples
  from a training set with randomly selected subset of attributes.
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r Initialize cache & seed, echo=FALSE}
knitr::opts_chunk$set(cache = TRUE)
set.seed(1234)
```

### Introduction
The purpose of this report is to summarize research that we did, the implementation that we committed and the tests that we performed as part of a MOW 2018L course project.

### The algorithm
The main goal of the assignment is briefly summarized in the subject of this paper. We will try to implement a classification model that make its prediction based on a set of internal models - so called ensemble learner. The flow of building and using such a classifier is presented in the picture below.

![](./assets/scheme.png)

First datasets will is splitted into training and testing parts. We will probably go with very natural 80 / 20 ratio.

Next we start building our complex classifier. Depending on the algorithm parameter values, N independent dataset samples will be choosen from the train set, each of parametrized **quantity M**. In order to choose observations for samples, a simple draw with returning algorithm is used. For each sample a random subset of **K features** is taken. Having all that bootstrapped we will train a model C N times on all prepared sample datasets. The combined classifier makes predictions for all pre-trained models - it is assumed that all submodels output one value with a predicted class. For each output a predicted class is onehot encoded. All predictions are summed. Based on a constructed vector of class prediction frequencies, an ensemble classifier outputs its prediction by taking the most frequent class as the final one.

Various simple models can be considered as a kernel for our ensemble. We choose just a few simple, already implemented packages and integrated them into the core of the algorithm:

* Recursive Partitioning - The algorithm creates a single decision tree, that tries to correctly classify members of the population by splitting it into sub-populations (through each of tree internal nodes) based on several independent variables. However, it does not perform well for continuous data, thus discretization can be needed. The implementation is available as a part of rpart or ctree CRAN packages.
* Naive Bayes - The classifier is based on a simple probabilistic theorem called the Bayesian theorem. An application is based on an assumption that the features of an observation contribute independently to the probability of classifying it to a certain class, regardless of any possible correlations between them. This does not happen very often, so the classifier is likely to not perform very well. The algorithm is implemented in e1071 CRAN package. 

To create models with the *naiveBayes* and *rpart* functions we require only the formula containing arguments to predict from and the training data set.

Our implementation allows any kind of parametrization of the submodels with their metaparameters. The specifics of the proposed interfaces as well as our suggestions for changes in metaconfiguration of the submodels is more deeply discussed in the *Experiments* section.

### Data Sets
To do some testing we agreed on using three different data sets available in the UCI repository. They are as follows:

* Cover Type - Data set with information about forests and their surroundings. Each record belongs to the one of 7 cover types and has 14 attributes like distance to hydrology and roadways, slope in degrees, elevation in meters and soil type. The soil type is determined by 40 binary variables indicating whether the soil belongs to a given class. During the experiment we will consider a conversion of the mentioned columns into one with 40 possible values each dedicated to the one soil type.
* Letter recognition –describes images of capital Latin letters in 16 numerical attributes (mainly statistical moments) which were scaled into integer values from 0 to 15. We will build models deciding what letter an image contains.
* Default of credit card clients - contains information about bank clients such as gender, education, payment history and bill statement. During the experiment we will predict if a client has a default payment in current month.  The class distribution of given records is not symmetrical(ratio 1:4), we will check whether the aliment of classes proportions in training data improves the models’ effectiveness.

### Other algorithms
As a counterexample we take some other complex classification algorithms and evaluate them on the same data (the same datasets, the same cleaning applied) as we evaluate our implementation. We tried:

* Bagging (bootstrap aggregating) - ensemble learning method very similar to the one that we are going to implement. We use ipred CRAN package with following arguments: number of bagging replication and number of sample to draw from the learning sample (if small algorithm uses sampling without replacement). Since this is meta-classification algorithm without any specific kernel defined we can use the same as for our implementation or try some different formulas.
* Random Forest - another ensemble learning method, based on a recursive partitioning kernel. The algorithm builds a lot of different decision trees (constructed with different parameters). Then the classification decision is based on a decision of all the individual submodels. Normally the mean or mode value is taken. The method can perform significantly better that a single tree, since it reduces overfitting. An implementation from randomForest CRAN package will be used with such arguments as farmula, vector indicating which rows belongs to test set, number of arguments that are choose at each split, numbers of trees to grow and maximum number of terminal nodes.

Suppose that we have our data cleaned and all our algorithms trained, we need to compare the performance of them. In order to do so we use two different metrics:
* simple prediction accuracy on a test set value,
* F1-score - harmonic mean of precission and recall values of a classifier from the interval of [0, 1], where 0 means the worst classifier and 1 means the best (in terms of precission and recall). We can consider calculating it with an external package such as MLmetrics, just for simplicity.

### Experiments
We start experiments section with loading necessary external libraries announced in the previous sections:
```{r Load external dependencies}
# install.packages('caret')
# install.packages('rpart')
# install.packages('e1071')
# install.packages('MLmetrics')

library('caret')
library('rpart')
library('e1071')
library('MLmetrics')
```

We experienced some issues with R, enforcing scientific notation, therefore we globally turn it off:
```{r Turning off scientific notation}
options(scipen = 999)
```

Now, let's load our scripts for preparing an ensemble classifier, making ensemble predictions and automating training/testing procedure.
Inspired by public implementations of popular classification algorithms we decided to split the interface into training and prediction parts.
Also we tried to design as usable interface as it was possible...

The **training function** expects to be parametrized with:

* a pointer to submodel function wrapper *(model)*,
* number of ensemble *submodel* classifier copies *(nsets)*,
* number of samples choosen for testing *(mset)*,
* number of features randomly choosen for each *submodel* *(nfeatures)*,
* a name of y column in train data set *(ycolname)*,
* a train dataset *(data)*.

The **predicting function** expects to be parametrized with:

* a trained ensemble classifier object of type returned by the training function *(object)*,
* a test dataset *(newdata)*,
* a type of outputted prediction - currently only *class* type is supported *(type)*.
```{r Load ensemble sources}
source('./R/ensemble.R')
source('./R/predict.ensemble.R')
source('./R/onehot.encode.R')
```

Soon after, the scripts for cleaning and loading experimental data sets should be loaded. Since we are using models, which are not affected by unnormalized data - normalization process is not performed by the scripts.
```{r Load datasets sources }
source('./R/datasets/covtype.R')
source('./R/datasets/default-of-credit-card-clients.R')
source('./R/datasets/letter-recognition.R')
```

In order to ease experience of executing our model for different sets of parameters we prepared a function which can be parametrized with sets of parameters for executing the **training function**, then it prepares a cross product of all sets, trains the model **ten times** for every configuration and logs averaged results to the *logs* directory. For each configuration the following metadata is logged:

* training execution time,
* testing execution time,
* accuracy score,
* F1 score.

Based on those statistics we will try to asses how well each model performs and compare it with the baseline (*bagging*, *randomForest*).

```{r Load helper sources}
source('./R/train.and.test.R')
```

Next we will prepare a function which uses the mechanism for a set of choosen simple model configurations described above.
We decided to try three rpart-based (*standard*, *complex*, *simple*) and three naiveBayes-based ensemble classifiers (*standard*, *1.0 Laplace smoothing*, *10.0 Laplace smoothing*).

```{r Train & Test ensemble models for data set}
train_and_test_ensemble <- function(df, df_name, ycolname) {
  classifier_funct <- function(
    data, nsets, mset, nfeatures,
    ycolname, xcolnames, formula,
    submodel_funct, replace, node_size) {
      ensemble(
        model = submodel_funct,
        nsets = nsets,
        mset = mset,
        nfeatures = nfeatures,
        ycolname = ycolname,
        data = data)
  }
  
  # rpart
  rpart_fun <- function(formula, data)
    rpart(formula, data = data)
  train_and_test(df, df_name, ycolname,
                 'rpart', classifier_funct, rpart_fun) 
  
  # more complex
  rpart_cms10_ccp1_fun <- function(formula, data)
    rpart(formula, data = data, control = rpart.control(minsplit = 10, cp = 1))
  
  train_and_test(df, df_name, ycolname,
                 'rpart_cms10_ccp1', classifier_funct, rpart_cms10_ccp1_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7)) 
  
  # less complex
  rpart_cms30_ccp0.001_cmd15_fun <- function(formula, data)
    rpart(formula, data = data, control = rpart.control(minsplit = 30, cp = 0.001, maxdepth = 15))
  train_and_test(df, df_name, ycolname,
                 'rpart_cms30_ccp0001_cmd15', classifier_funct, rpart_cms30_ccp0.001_cmd15_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7)) 
  
  # naiveBayes
  naiveBayes_fun <- function(formula, data)
    naiveBayes(formula, data = data)
  train_and_test(df, df_name, ycolname,
                 'naivebayes', classifier_funct, naiveBayes_fun) 
  
  # natural Laplace smoothing
  naiveBayes_l1_fun <- function(formula, data)
    naiveBayes(formula, data = data, laplace = 1.0)
  train_and_test(df, df_name, ycolname,
                 'naivebayes_l1', classifier_funct, naiveBayes_l1_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7)) 
  
  # Laplace smoothing with higher alpha
  naiveBayes_l10_fun <- function(formula, data)
    naiveBayes(formula, data = data, laplace = 10.0)
  train_and_test(df, df_name, ycolname,
                 'naivebayes_l10', classifier_funct, naiveBayes_l10_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7))
}
```

We will also need a similar function for randomForest. Apart from the standard version we will try to test: an overriden value for nodesize and turn sampling with replacement on.

```{r Train & Test randomForest models for data set}
train_and_test_randomforest <- function(df, df_name, ycolname, rpart_funct) {
  randomForest_fun <- function(data, nsets, mset, nfeatures,
                               ycolname, xcolnames, formula,
                               submodel_funct) {
    randomForest(x = data[, xcolnames], y = data[, ycolname],
                 ntree = nsets, mtry = nfeatures,
                 sampsize = mset)
  }
  
  train_and_test(df, df_name, ycolname,
                 'randomForest', randomForest_fun)
    
  # minimum node size
  randomForest_ns10_func <- function(data, nsets, mset, nfeatures,
                                     ycolname, xcolnames, formula,
                                     submodel_funct) {
    randomForest(x = data[, xcolnames], y = data[, ycolname],
                 ntree = nsets, mtry = nfeatures,
                 sampsize = mset, nodesize = 10)
  }
  
  train_and_test(df, df_name, ycolname,
                 'randomForest', randomForest_ns10_func)

  # sampling with replacement
  randomForest_r1_func <- function(data, nsets, mset, nfeatures,
                                   ycolname, xcolnames, formula,
                                   submodel_funct) {
    randomForest(x = data[, xcolnames], y = data[, ycolname],
                 ntree = nsets, mtry = nfeatures,
                 sampsize = mset, replace = 1)
  }
  
  train_and_test(df, df_name, ycolname,
                 'randomForest', randomForest_r1_func)
}
```

And the same for bagging...

```{r Train & Test bagging models for data set}
train_and_test_bagging <- function(df, df_name, ycolname) {
  classifier_funct <- function(data, nsets, mset, nfeatures,
                               ycolname, xcolnames, formula,
                               submodel_funct) {
    bagging(data = data, formula = formula,
            nbagg = nsets, ns = mset)
  }
  
  train_and_test(df, df_name, ycolname, 'bagging', classifier_funct)
}
```

Now, let's load datasets and execute prepared functions.

###### Letter recognition (LR)
```{r Letter recognition, eval=FALSE}
lr_df <- load_lr_df()
sapply(lr_df, class)
summary(lr_df)

train_and_test_ensemble(lr_df, 'lr', 'lettr')
train_and_test_bagging(lr_df, 'lr', 'lettr')
train_and_test_randomforest(lr_df, 'lr', 'lettr')
```

###### Default of credit card clients (DCCC)
```{r Default of credit card clients, eval=FALSE}
dccc_df <- load_dccc_df()
sapply(dccc_df, class)
summary(dccc_df)

train_and_test_ensemble(dccc_df, 'dccc', 'DEFAULT_PAY')
train_and_test_bagging(dccc_df, 'dccc', 'DEFAULT_PAY')
train_and_test_randomforest(dccc_df, 'dccc', 'DEFAULT_PAY')
```

###### Cover types (CT)
```{r Cover types, eval=FALSE}
covtype_df <- load_cr_df()
sapply(covtype_df, class)
summary(covtype_df)

train_and_test_ensemble(covtype_df, 'covtype', 'Cover_Type')
train_and_test_bagging(covtype_df, 'covtype', 'Cover_Type')
train_and_test_randomforest(covtype_df, 'covtype', 'Cover_Type')
```

### Summary
Since *train and test* results are already logged we can proceed with analysis. We will start with loading the essential visualization and data processing libraries. We will also load and concatenate all logs into one dataframe.
```{r Load logs, results=FALSE}
# install.packages('tidyverse')
# install.packages('dplyr')
library('tidyverse')
library('dplyr')

# concat all logs
log_filenames <- dir('log') %>% map(function(f) paste0('log/', f))
log_colnames <- c('df_name', 'submodel_name', 'nsets', 'mset', 'nfeatures',
                  'train_time', 'test_time', 'accuracy', 'f1_score')
log_df <- do.call(rbind, lapply(log_filenames, function(f)
  read.table(f, header = FALSE, sep = '|', col.names = log_colnames))) %>% data.frame()
```

##### Overall
The following table presents the best models sorted in descending manner by their F1 score, averaged over all configurations and all data sets. 
```{r Overall accuracy and f1_score for each model}
log_df_agg <- log_df
log_df_agg[str_detect(log_df_agg$submodel_name, 'bagging'),]$submodel_name <- 'bagging'
log_df_agg[str_detect(log_df_agg$submodel_name, 'randomForest'),]$submodel_name <- 'randomForest'
log_df_agg[str_detect(log_df_agg$submodel_name, 'naivebayes'),]$submodel_name <- 'naivebayes'
log_df_agg[str_detect(log_df_agg$submodel_name, 'rpart'),]$submodel_name <- 'rpart'
log_df_agg %>% group_by(submodel_name) %>% summarise(train_time = mean(train_time), test_time = mean(test_time), accuracy = mean(accuracy), f1_score = mean(f1_score)) %>% arrange(desc(f1_score))
```
It seams that in terms of accuracy and F1 score our ensemble model is not better than native implementations of *randomForest* and *rpart*.
Noticable difference between training and testing times for *naivebayes* and *rpart* should be pointed here. Interestingly despite the fact that *rpart* was trained much longer it yealds worse scores.

However, such aggregated results can quickly estimate general performance of the models over huge space of different configurations it can be very misleading. It can happen that only a few configurations of ensemble worsen the overal score. Let's try to get a bit deeper into generated meta data and try to find any applications where our implementation can be considered better than the baseline.

How about considering the DCCC set only?
```{r Dccc accuracy and f1_score for each model, collapse=TRUE}
log_df_agg <- log_df[log_df$df_name == 'dccc',]
log_df_agg[str_detect(log_df_agg$submodel_name, 'bagging'),]$submodel_name <- 'bagging'
log_df_agg[str_detect(log_df_agg$submodel_name, 'randomForest'),]$submodel_name <- 'randomForest'
log_df_agg[str_detect(log_df_agg$submodel_name, 'naivebayes'),]$submodel_name <- 'naivebayes'
log_df_agg[str_detect(log_df_agg$submodel_name, 'rpart'),]$submodel_name <- 'rpart'
log_df_agg %>% group_by(submodel_name) %>% summarise(train_time = mean(train_time), test_time = mean(test_time), accuracy = mean(accuracy), f1_score = mean(f1_score)) %>% arrange(desc(f1_score))
```

The performance gap seems to be significantly lower than in the previous example. rpart ensemble seems to be performing surprisingly good for credit card defaults data.

#### Details
We tried to investigate the results in depth by plotting and analyzing a few of available log profiles.

For instance, in the following figures we can observe an obvious positive corrlation between accuracy or F1 score and the number of submodels in the ensemble. For *randomForest* the nsets number is considered as the number of trees used. For *bagging* - number of bag samples.

![](assets/dccc_m1000_k5_n-all_acc.png){ width=49% }
![](assets/dccc_m1000_k5_n-all_f1.png){ width=49% }

Generally, all models for DCCC and LR datasets tend to behave better if we include more features in training phase. This is easily noticable in the next four figures.

![](assets/dccc_naivebayes_m1000_n-k_f1.png){ width=49% }
![](assets/dccc_rpart_m1000_n-k_f1.png){ width=49% }
![](assets/lr_naivebayes_m1000_n-k_f1.png){ width=49% }
![](assets/lr_randomForest_m1000_n-k_f1.png){ width=49% }

All prepared plots can be found in the *'assets'* folder of this repository.

### Conclusion

We succedeed on the project goal which was to implement a generic ensemble solution. The developed package can be used in the future in rapid prototyping of simple ensemble models, using different kernel models. What is more, for certin results, our implementation performed nearly as good as the baseline. We also confirmed our early assumption, that generally increasing the value of the defined parameters: **n, m, k** increases the overall performance of the model.

### References

* https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/
* Ensamble learners
https://www.youtube.com/watch?v=Un9zObFjBH0
* Bootstrap aggregating bagging
https://www.youtube.com/watch?v=2Mg8QD0F1dQ
* Bagging
https://www.youtube.com/watch?v=sVriC_Ys2cw
* Random Forest
https://en.wikipedia.org/wiki/Random_forest 
* Recursive Partitioning
** https://en.wikipedia.org/wiki/Recursive_partitioning
** https://cran.r-project.org/web/packages/rpart/rpart.pdf#rpart
* Naive Bayes
** https://cran.r-project.org/web/packages/e1071/e1071.pdf#naiveBayes
