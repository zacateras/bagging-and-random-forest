---
title: "MOW - REPORT"
author: "Michał Sitko, Martyna Kania"
subtitle: Ensemble model - applying any classification algorithm to bootstrap samples
  from a training set with randomly selected subset of attributes.
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r Initialize cache & seed, echo=FALSE}
knitr::opts_chunk$set(cache = TRUE)
set.seed(1234)
```

### Introduction
The purpose of this report is to summarize research that we did, the implementation that we committed and the tests that we performed as part of a MOW 2018L course project.

### The algorithm
The main goal of the assignment is briefly summarized in the subject of this paper. We will try to implement a classification model that make its prediction based on a set of internal models - so called ensemble learner. The flow of building and using such a classifier is presented in the picture below.

![](./assets/scheme.png)

First datasets will be splitted into training and testing parts. We will probably go with very natural 80 / 20 ratio.

Next we will start building our complex classifier. Depending on the algorithm parameter values, N independent dataset samples will be generated, each of parametrized quantity M. In order to choose observations for samples, simple draw with returning algorithm will be used. For each sample a random subset of K features will be taken. Having all that bootstrapped we will train a model C N times on all prepared sample datasets. The combined classifier will make predictions for all pre-trained models and output the most frequent prediction as the final one.

Various simple internal models can be considered. We will choose just a few simple already implemented packages and integrate them into the core of the algorithm:

* Recursive Partitioning - The algorithm creates a single decision tree, that tries to correctly classify members of the population by splitting it into sub-populations (through each of tree internal nodes) based on several independent variables. However, it does not perform well for continuous data, thus discretization can be needed. The implementation is available as a part of rpart or ctree CRAN packages.
* Naive Bayes - The classifier is based on a simple probabilistic theorem called the Bayesian theorem. An application is based on an assumption that the features of an observation contribute independently to the probability of classifying it to a certain class, regardless of any possible correlations between them. This does not happen very often, so the classifier is likely to not perform very well. The algorithm is implemented in e1071 CRAN package. 

To create models with the *naiveBayes* and *rpart* functions we will only need formula containing arguments to predict from and training data set.

If it will be possible, our implementation will provide an interface for passing metaparameters for its internal models. The space of different parameter values will be evaluated in order to find an optimal combination.

### Data Sets
To do some testing we agreed on using three different data sets available in the UCI repository. They are as follows:

* Cover Type - Data set with information about forests and their surroundings.  Each record belongs to the one of 7 cover types and has 14 attributes like distance to hydrology and roadways, slope in degrees, elevation in meters and soil type. The soil type is determined by 40 binary variables indicating whether the soil belongs to a given class. During the experiment we will consider a conversion of the mentioned columns into one with 40 possible values each dedicated to the one soil type.
* Letter recognition –  describes images of capital Latin letters in 16 numerical attributes (mainly statistical moments) which were scaled into integer values from 0 to 15. We will build models deciding what letter an image contains.
* Default of credit card clients - contains information about bank clients such as gender, education, payment history and bill statement. During the experiment we will predict if a client has a default payment in current month.  The class distribution of given records is not symmetrical(ratio 1:4), we will check whether the aliment of classes proportions in training data improves the models’ effectiveness.

### Other algorithms
As a counterexample we will take some other complex classification algorithms and try to evaluate them on the same data (the same datasets, the same cleaning applied) as we will evaluate our implementation. We will try:

* Bagging (bootstrap aggregating) - ensemble learning method very similar to the one that we are going to implement. We will use ipred CRAN package with following arguments: number of bagging replication and number of sample to draw from the learning sample (if small algorithm uses sampling without replacement). Since this is meta-classification algorithm without any specific kernel defined we can use the same as for our implementation or try some different formulas.
* Random Forest - another ensemble learning method, based on a recursive partitioning kernel. The algorithm builds a lot of different decision trees (constructed with different parameters). Then the classification decision is based on a decision of all the individual submodels. Normally the mean or mode value is taken. The method can perform significantly better that a single tree, since it reduces overfitting. An implementation from randomForest CRAN package will be used with such arguments as farmula, vector indicating which rows belongs to test set, number of arguments that are choose at each split, numbers of trees to grow and maximum number of terminal nodes.

Suppose that we will have our data cleaned and all our algorithms trained, we will need to compare the performance of them. In order to do so we will use two different metrics:
* simple prediction accuracy on a test set value,
* F1-score - harmonic mean of precission and recall values of a classifier from the interval of [0, 1], where 0 means the worst classifier and 1 means the best (in terms of precission and recall). We can consider calculating it with an external package such as MLmetrics, just for simplicity.

### Experiments
Let's start experiments section with loading necessary external libraries announced in the proceeding sections:
```{r Load external dependencies}
# install.packages('caret')
# install.packages('rpart')
# install.packages('e1071')
# install.packages('MLmetrics')

library('caret')
library('rpart')
library('e1071')
library('MLmetrics')
```

We experienced some issues with R, enforcing scientific notation, therefore we globally turn it off:
```{r Turning off scientific notation}
options(scipen = 999)
```

Now, let's load our scripts for preparing an ensemble classifier, making ensemble predictions and automating training/testing procedure.
Inspired by public implementations of popular classification algorithms we decided to split the interface into training and prediction parts.
Also we tried to design as usable interface as it was possible...

The **training function** expects to be parametrized with:

* a pointer to submodel function wrapper *(model)*,
* number of ensemble *submodel* classifier copies *(nsets)*,
* number of samples choosen for testing *(mset)*,
* number of features randomly choosen for each *submodel* *(nfeatures)*,
* a name of y column in train data set *(ycolname)*,
* a train dataset *(data)*.

The **predicting function** expects to be parametrized with:

* a trained ensemble classifier object of type returned by the training function *(object)*,
* a test dataset *(newdata)*,
* a type of outputted prediction - currently only *class* type is supported *(type)*.
```{r Load ensemble sources}
source('./R/ensemble.R')
source('./R/predict.ensemble.R')
source('./R/onehot.encode.R')
```

Soon after, the scripts for cleaning and loading experimental data sets should be loaded. Since we are using models which are not affected by unnormalized data normalization process is not performed by the scripts.
```{r Load datasets sources }
source('./R/datasets/covtype.R')
source('./R/datasets/default-of-credit-card-clients.R')
source('./R/datasets/letter-recognition.R')
```

In order to ease experience of executing our model for different sets of parameters we prepared a function which can be parametrized with sets of parameters for executing the **training function**, then it prepares a cross product of all sets, trains the model for every configuration and logs results to the *logs* directory. For each configuration the following metadata is logged:

* training execution time,
* testing execution time,
* accuracy score,
* F1 score.

Based on those statistics we will try to asses how well each model performs and compare it with the baseline (*bagging*, *randomForest*).

```{r Load helper sources}
source('./R/train.and.test.R')
```

Let's prepare now a function which will use the above described mechanism for a set of choosen simple model configurations.
We decided to try three rpart-based (*standard*, *complex*, *simple*) and three naiveBayes-based ensemble classifiers (*standard*, *1.0 Laplace smoothing*, *10.0 Laplace smoothing*).

```{r Train & Test ensemble models for data set}
train_and_test_ensemble <- function(df, df_name, ycolname) {
  classifier_funct <- function(
    data, nsets, mset, nfeatures,
    ycolname, xcolnames, formula,
    submodel_funct, replace, node_size) {
      ensemble(
        model = submodel_funct,
        nsets = nsets,
        mset = mset,
        nfeatures = nfeatures,
        ycolname = ycolname,
        data = data)
  }
  
  # rpart
  rpart_fun <- function(formula, data)
    rpart(formula, data = data)
  train_and_test(df, df_name, ycolname,
                 'rpart', classifier_funct,
                 "source('./R/predict.ensemble.R')", rpart_fun) 
  
  # more complex
  rpart_cms10_ccp1_fun <- function(formula, data)
    rpart(formula, data = data, control = rpart.control(minsplit = 10, cp = 1))
  
  train_and_test(df, df_name, ycolname,
                 'rpart_cms10_ccp1', classifier_funct,
                 "source('./R/predict.ensemble.R')", rpart_cms10_ccp1_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7)) 
  
  # less complex
  rpart_cms30_ccp0.001_cmd15_fun <- function(formula, data)
    rpart(formula, data = data, control = rpart.control(minsplit = 30, cp = 0.001, maxdepth = 15))
  train_and_test(df, df_name, ycolname,
                 'rpart_cms30_ccp0001_cmd15', classifier_funct,
                 "source('./R/predict.ensemble.R')", rpart_cms30_ccp0.001_cmd15_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7)) 
  
  # naiveBayes
  naiveBayes_fun <- function(formula, data)
    naiveBayes(formula, data=data)
  train_and_test(df, df_name, ycolname,
                 'naivebayes', classifier_funct,
                 "source('./R/predict.ensemble.R')", naiveBayes_fun) 
  
  # natural Laplace smoothing
  naiveBayes_l1_fun <- function(formula, data)
    naiveBayes(formula, data = data, laplace = 1.0)
  train_and_test(df, df_name, ycolname,
                 'naivebayes_l1', classifier_funct,
                 "source('./R/predict.ensemble.R')", naiveBayes_l1_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7)) 
  
  # Laplace smoothing with higher alpha
  naiveBayes_l10_fun <- function(formula, data)
    naiveBayes(formula, data=data, laplace=10.0)
  train_and_test(df, df_name, ycolname,
                 'naivebayes_l10', classifier_funct, 
                 "source('./R/predict.ensemble.R')", naiveBayes_l10_fun,
                 model_nset = c(10, 100), model_nfeatures = c(3, 7))
}
```

```{r Train & Test bagging models for data set}
train_and_test_bagging <- function(df, df_name, ycolname) {
  classifier_funct <- function(data, nsets, mset, nfeatures,
                               ycolname, xcolnames, formula,
                               submodel_funct) {
    bagging(data = data, formula = formula,
            nbagg = nsets, ns = mset)
  }
  
  train_and_test(df, df_name, ycolname, 'bagging',
                 classifier_funct, "library('ipred')")
}
```

```{r Train & Test randomForest models for data set}
train_and_test_randomforest <- function(df, df_name, ycolname, rpart_funct) {
  randomForest_fun <- function(data, nsets, mset, nfeatures,
                               ycolname, xcolnames, formula,
                               submodel_funct) {
    randomForest(x = data[, xcolnames], y = data[, ycolname],
                 ntree = nsets, mtry = nfeatures,
                 sampsize = mset)
  }
  
  train_and_test(df, df_name, ycolname,
                 'randomForest', randomForest_fun,
                 "library('randomForest')")
    
  # minimum node size
  randomForest_ns10_func <- function(data, nsets, mset, nfeatures,
                                     ycolname, xcolnames, formula,
                                     submodel_funct) {
    randomForest(x = data[, xcolnames], y = data[, ycolname],
                 ntree = nsets, mtry = nfeatures,
                 sampsize = mset, nodesize = 10)
  }
  
  train_and_test(df, df_name, ycolname,
                 'randomForest', randomForest_ns10_func,
                 "library('randomForest')")

  # sampling with replacement
  randomForest_r1_func <- function(data, nsets, mset, nfeatures,
                                   ycolname, xcolnames, formula,
                                   submodel_funct) {
    randomForest(x = data[, xcolnames], y = data[, ycolname],
                 ntree = nsets, mtry = nfeatures,
                 sampsize = mset, replace = 1)
  }
  
  train_and_test(df, df_name, ycolname,
                 'randomForest', randomForest_r1_func,
                 "library('randomForest')")
}
```

```{r Letter recognition}
lr_df <- load_lr_df()
sapply(lr_df, class)
summary(lr_df)
# train_and_test_ensemble(lr_df, 'lr', 'lettr')
# train_and_test_bagging(lr_df, 'lr', 'lettr')
# train_and_test_randomforest(lr_df, 'lr', 'lettr')
```

```{r Default of credit card clients}
dccc_df <- load_dccc_df()
sapply(dccc_df, class)
summary(dccc_df)
# train_and_test_ensemble(dccc_df, 'dccc', 'DEFAULT_PAY')
# train_and_test_bagging(dccc_df, 'dccc', 'DEFAULT_PAY')
# train_and_test_randomforest(dccc_df, 'dccc', 'DEFAULT_PAY')
```

```{r Cover types}
covtype_df <- load_cr_df()
sapply(covtype_df, class)
summary(covtype_df)
# train_and_test_ensemble(covtype_df, 'covtype', 'Cover_Type')
# train_and_test_bagging(covtype_df, 'covtype', 'Cover_Type')
# train_and_test_randomforest(covtype_df, 'covtype', 'Cover_Type')
```

```{r Load logs}
# install.packages('tidyverse')
# install.packages('dplyr')
library('tidyverse')
library('dplyr')

# concat all logs
log_filenames <- dir('log') %>% map(function(f) paste0('log/', f))
log_colnames <- c('df_name', 'submodel_name', 'nsets', 'mset', 'nfeatures', 'train_time', 'test_time', 'accuracy', 'f1_score')
log_df <- do.call(rbind, lapply(log_filenames, function(f) read.table(f, header = FALSE, sep = '|', col.names = log_colnames)))
```

```{r Overall accuracy and f1_score for each model}
# overall accuracy and f1_score for each model
log_df %>% group_by(submodel_name) %>% summarise(train_time = mean(train_time), test_time = mean(test_time), accuracy = mean(accuracy), f1_score = mean(f1_score)) %>% arrange(desc(f1_score))
```

### References

* https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/
* Ensamble learners
https://www.youtube.com/watch?v=Un9zObFjBH0
* Bootstrap aggregating bagging
https://www.youtube.com/watch?v=2Mg8QD0F1dQ
* Bagging
https://www.youtube.com/watch?v=sVriC_Ys2cw
* Random Forest
https://en.wikipedia.org/wiki/Random_forest 
* Recursive Partitioning
** https://en.wikipedia.org/wiki/Recursive_partitioning
** https://cran.r-project.org/web/packages/rpart/rpart.pdf#rpart
* Naive Bayes
** https://cran.r-project.org/web/packages/e1071/e1071.pdf#naiveBayes
